<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="We propose a framework for context-aware robot navigation that excels in both simulated and real-world environments.">
    <meta name="keywords" content="Vision-Language-Action (VLA) Models, Imitation Learning, Multimodal Instruction Following">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5J9LZW868J"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-5J9LZW868J');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="../static/css/bulma.min.css">
    <link rel="stylesheet" href="../static/css/slick.css">
    <link rel="stylesheet" href="../static/css/slick-theme.css">
    <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="../static/css/index.css">
    <link rel="icon" href="./static/images/favicon.ico">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="../static/js/fontawesome.all.min.js"></script>
    <script src="../static/js/slick.min.js"></script>
    <script src="../static/js/index.js"></script>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <a class="navbar-item" href="https://worv-ai.github.io/">
                    <span class="icon">
                        <i class="fas fa-home"></i>
                    </span>
                </a>

                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://worv-ai.github.io/">
                            Not released yet
                        </a>
                    </div>
                </div>
            </div>

        </div>
    </nav>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">CANVAS: Commonsense-Aware Navigation System
                            for Intuitive Human-Robot Interaction</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                Suhwan Choi<sup>†1</sup>,</span>
                            <span class="author-block">
                                Yongjun Cho<sup>†1</sup>,</span>
                            <span class="author-block">
                                Minchan Kim<sup>†1</sup>,</span>
                            </span>
                            <span class="author-block">
                                Jaeyoon Jung<sup>†1</sup>,</span>
                            </span>
                            <span class="author-block">
                                Myunchul Joe<sup>1</sup>,</span>
                            </span>
                            <span class="author-block">
                                Yubeen Park<sup>1</sup>,</span>
                            </span>
                            <br>
                            <span class="author-block">
                                Minseo Kim<sup>2</sup>,</span>
                            </span>
                            <span class="author-block">
                                Sungwoong Kim<sup>2</sup>,</span>
                            </span>
                            <span class="author-block">
                                Sungjae Lee<sup>2</sup>,</span>
                            </span>
                            <span class="author-block">
                                Hwiseong Park<sup>1</sup>,</span>
                            </span>
                            <span class="author-block">
                                Jiwan Chung<sup>2</sup>,</span>
                            </span>
                            <span class="author-block">
                                Youngjae Yu<sup>2</sup>,</span>
                            </span>
                        </div>

                        <div class="is-size-6 publication-authors">
                            <span class="author-block">
                                <sup>†</sup> Equal contribution, <sup>1</sup> MAUM.AI, <sup>2</sup> Yonsei University
                        </div>
                        <br>
                        <div class="is-size-6 publication-venue">
                          <span class="venue-block">Under Review</span> <br>
                          <!-- <span class="venue-block">NeurIPS 2024 Workshop on Open-World Agents</span> -->
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2410.01273"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>
                                <!-- Video Link. -->
                                <span class="link-block">
                                    <a href="https://youtu.be/oJuU4x02azI"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                        <span>Video</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <!-- <span class="link-block">
                                    <a href="https://github.com/worv-ai/canvas"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github-alt"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span> -->
                                <!-- Model Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/worv-ai/canvas"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-robot"></i>
                                        </span>
                                        <span>Model</span>
                                    </a>
                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/worv-ai/canvas"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i> </span>
                                        <span>Data</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop is-centered has-text-justified is-size-5">
            <div class="hero-body">
                <figure id="teaser">
                    <img src="./static/1_teaser.png" alt="canvas teaser" />
                </figure>
                <p>
                    CANVAS github.io will be updated soon. Stay tuned for more information.
                </p>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>    
                            Real-life robot navigation involves more than just reaching a destination; 
                            it requires optimizing movements while addressing scenario-specific goals. 
                            An intuitive way for humans to express these goals is through abstract cues 
                            like verbal commands or rough sketches. Such human guidance may lack details 
                            or be noisy. Nonetheless, we expect robots to navigate as intended. 
                            For robots to interpret and execute these abstract instructions in line with 
                            human expectations, they must share a common understanding of basic navigation 
                            concepts with humans. 
                        </p>
                        <p>
                            To this end, we introduce CANVAS, a novel framework that combines visual and 
                            linguistic instructions for commonsense-aware navigation. Its success is driven 
                            by imitation learning, enabling the robot to learn from human navigation behavior. 
                            We present COMMAND, a comprehensive dataset with human-annotated navigation results, 
                            spanning over 48 hours and 219 km, designed to train commonsense-aware navigation systems 
                            in simulated environments. Our experiments show that CANVAS outperforms the strong 
                            rule-based system ROS NavStack across all environments, demonstrating superior performance 
                            with noisy instructions. Notably, in the orchard environment, where ROS NavStack records a 
                            <strong>0%</strong> total success rate, CANVAS achieves a total success rate of <strong>67%</strong>. CANVAS also closely 
                            aligns with human demonstrations and commonsense constraints, even in unseen environments. 
                            Furthermore, real-world deployment of CANVAS showcases impressive Sim2Real transfer with a 
                            total success rate of 69%, highlighting the potential of learning from human demonstrations 
                            in simulated environments for real-world applications
                        </p>
                    </div>
                </div>
            </div>

            <!--/ Abstract. -->

            <!-- Paper video. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Summary Video</h2>
                    <div class="publication-video">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/oJuU4x02azI?si=YRwnQb3IzmKfxbTo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">The COMMAND dataset</h2>
                    <div class="content has-text-justified">
                        <figure id="data_pipeline">
                            <img src="./static/2_data_pipeline.png" alt="data_pipeline" />
                        </figure>
                        <p>
                            The COMMAND dataset is a comprehensive dataset that includes human-annotated navigation results spanning over 48 hours and 219 kilometers, 
                            specifically designed to train commonsense-aware navigation systems in simulated environments.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">CANVAS model</h2>
                    <div class="content has-text-justified">
                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                            <tr>
                                <td width="50%">
                                    <figure id="model">
                                        <img src="./static/3_framework.png" alt="model" />
                                    </figure>
                                </td>
                                <td width="50%">
                                    <p>
                                        The CANVAS model is a novel framework that integrates both visual and linguistic instructions for commonsense-aware navigation.
                                    </p>
                                </td>
                            </tr>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@article{
                choi2024canvas,
                title={CANVAS: Commonsense-Aware Navigation System for Intuitive Human-Robot Interaction}, 
                author={Suhwan Choi and Yongjun Cho and Minchan Kim and Jaeyoon Jung and Myunchul Joe and Yubeen Park and Minseo Kim and Sungwoong Kim and Sungjae Lee and Hwiseong Park and Jiwan Chung and Youngjae Yu},
                year={2024}
            }</code></pre>
        </div>
    </section>
    <br>
    <center class="is-size-10">
        The website design was based on <a href="https://github.com/general-navigation-models/general-navigation-models.github.io"><span 
        class="dnerf">general-navigation-models</span></a> adapted from <a href="https://nerfies.github.io" class="external-link"><span
        class="dnerf">Nerfies</span></a>.
    </center>
    <br>
</body>

</html>
